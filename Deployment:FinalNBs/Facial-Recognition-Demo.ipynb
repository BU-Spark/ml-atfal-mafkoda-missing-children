{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa0ac0b7",
   "metadata": {},
   "source": [
    "# Gradio Demo For Facial Recognition on Missing Children Database"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffa4f55",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8e14834",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure to import these dependencies in this order\n",
    "import os\n",
    "import gradio as gr\n",
    "from align import align_face\n",
    "from PIL import Image\n",
    "os.chdir('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88434041",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation Models: using `tf.keras` framework.\n"
     ]
    }
   ],
   "source": [
    "#may or may not need to comment the following out \n",
    "import os\n",
    "os.environ[\"SM_FRAMEWORK\"] = \"tf.keras\"\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['NUMBAPRO_NVVM']='/share/pkg.7/cuda/11.2/install/nvvm/lib64/libnvvm.so'\n",
    "os.environ['NUMBAPRO_LIBDEVICE']='/share/pkg.7/cuda/11.2/install/nvvm/libdevice/'\n",
    "import segmentation_models as sm\n",
    "os.environ['TF_ENABLE_ONEDNN_OPTS'] = '0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b7b4b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras_vggface\n",
    "from keras_vggface.vggface import VGGFace\n",
    "import mtcnn\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "from keras.utils.data_utils import get_file\n",
    "import dlib\n",
    "import keras_vggface.utils\n",
    "import PIL\n",
    "import os\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f95dfa0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "from deepface import DeepFace\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fb1522",
   "metadata": {},
   "source": [
    "## Create Gradio Interface and Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d98dc823",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "  \"VGG-Face\", \n",
    "  \"Facenet\", \n",
    "  \"Facenet512\", \n",
    "  \"OpenFace\", \n",
    "  \"DeepFace\", \n",
    "  \"DeepID\", \n",
    "  \"ArcF/ace\", \n",
    "  \"Dlib\", \n",
    "  \"SFace\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad7b1d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "#you will need to specify the path to your db\n",
    "def db_find(path, frontalize, db=\"atfalmafkoda_unzip/database3\", model=0, thresh=.2):\n",
    "    \"\"\"function to be passed into gradio that will check input image against the database and find matches\n",
    "    args:\n",
    "        path- path to image; can be numpy array as well\n",
    "        align- bool; if True, frontalize image\n",
    "        db- path to database\n",
    "        model- 0-8; will load different models shown above (Dlib does not work)\n",
    "        thresh- threshold to return images\n",
    "    output:\n",
    "        returns a dataframe of the matched images and a gallery of the actual images\n",
    "    \"\"\"\n",
    "    if frontalize:\n",
    "        print(type(path))\n",
    "        path = align_face(path)[0]\n",
    "        print(type(path))\n",
    "    m = model\n",
    "    dfs = DeepFace.find(img_path=path, db_path=db, model_name=models[m], detector_backend=\"mtcnn\", enforce_detection=False)\n",
    "    df = dfs[0].copy()\n",
    "    df = df.drop(columns=['source_x', 'source_y', 'source_w', 'source_h'])\n",
    "    df['id'] = df['identity'].str.strip(\"atfalmafkoda_unzip/database/person\").str.split(\"/\")\n",
    "    df['id'] = df['id'].apply(lambda x: x[0])\n",
    "    img_len = df.loc[df[\"VGG-Face_cosine\"] < thresh].shape[0]\n",
    "    imgs = df.head(img_len)['identity'].tolist()\n",
    "    return df.loc[df[\"VGG-Face_cosine\"] < thresh], imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2eddc29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTANT: You are using gradio version 3.9, however version 3.14.0 is available, please upgrade.\n",
      "--------\n",
      "Running on local URL:  http://0.0.0.0:7860\n",
      "Running on public URL: https://4e40cd6582c174a1.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://4e40cd6582c174a1.gradio.app\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x2ad6f232e5b0>,\n",
       " 'http://localhost:7860/',\n",
       " 'https://4e40cd6582c174a1.gradio.app')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Representations for images in atfalmafkoda_unzip/database3 folder were previously stored in representations_vgg_face.pkl. If you added new instances after the creation, then please delete this file and call find function again. It will create it again.\n",
      "There are  6707  representations found in  representations_vgg_face.pkl\n",
      "1/1 [==============================] - 6s 6s/step\n",
      "1/1 [==============================] - 1s 642ms/step\n",
      "1/1 [==============================] - 0s 301ms/step\n",
      "1/1 [==============================] - 0s 169ms/step\n",
      "1/1 [==============================] - 0s 103ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "2/2 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 125ms/step\n",
      "find function lasts  25.706911325454712  seconds\n"
     ]
    }
   ],
   "source": [
    "# code to run gradio demo\n",
    "demo = gr.Interface(fn=db_find, inputs=[\"image\", gr.Checkbox(label=\"align\")], outputs=[\"dataframe\", \"gallery\"])\n",
    "demo.launch(server_name='0.0.0.0', share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
